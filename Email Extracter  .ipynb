{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "from scrapy.selector import Selector\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_from_web(X):\n",
    "    try:\n",
    "        url=[]\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36\"}\n",
    "        web=str('http://www.'+str(X))\n",
    "        url.append(web)\n",
    "        #-------------fetch all the links from website and filtter only contect and about link----------------------\n",
    "        links=[]\n",
    "        req = Request(web,headers=headers)\n",
    "        html_page = urlopen(req)\n",
    "        soup = BeautifulSoup(html_page, \"lxml\")\n",
    "        for link in soup.findAll('a'):\n",
    "            links.append(link.get('href'))\n",
    "        #-----------------------------------------------------------------------------------------------------------    \n",
    "        url_list=[]\n",
    "        for i in range(0,len(links),1):\n",
    "            if \"contact\" in str(links[i]).lower():\n",
    "                url_list.append(web+str(links[i]))\n",
    "            if \"about\" in str(links[i]).lower():\n",
    "                url_list.append(web+str(links[i]))\n",
    "        #--------------------------------------------------- ---------------------------------------------------------         \n",
    "        #print(url_list)\n",
    "        for i in url_list:\n",
    "            if str(web[-3:]+'http') in i:\n",
    "                url.append('http'+i.split(web[-3:]+'http')[1])\n",
    "            if i not in url: \n",
    "                url.append(i)   \n",
    "        #print(url)        \n",
    "        #--------------------------section 2 get response and fetch data-----------------------------------------------\n",
    "        emails=[]\n",
    "        for i in tqdm(url):\n",
    "            try:\n",
    "                response=requests.get(i,headers=headers)\n",
    "            except:\n",
    "                  pass\n",
    "#                 print('response time error')\n",
    "            if str(response)==\"<Response [200]>\":\n",
    "                #print(i)\n",
    "                sel=Selector(text=response.text)\n",
    "                email_1=sel.xpath('//a[contains(@href,\"mailto\")]/@href').extract()\n",
    "                email_2=sel.xpath('//*[contains(text(),\"info@\")]/text()').extract()\n",
    "                email_3=sel.xpath('//*[contains(text(),\"@gmail\")]/text()').extract_first()\n",
    "                email_4=sel.xpath('//*[contains(text(),\"@aol\")]/text()').extract_first()\n",
    "                email_5=sel.xpath('//*[contains(text(),\"@hotmail\")]/text()').extract_first()\n",
    "                email_6=sel.xpath('//*[contains(text(),\"@yahoo\")]/text()').extract_first()\n",
    "                \n",
    "                #---------------section 3 store into list-------------------------------\n",
    "                emails.extend(email_1)\n",
    "                emails.extend(email_2)\n",
    "                emails.append(email_3)\n",
    "                emails.append(email_4)\n",
    "                emails.append(email_5)\n",
    "                emails.append(email_6)    \n",
    "        #----------------preprocess the data and return Email if its present------------------------------------------\n",
    "        res = []    \n",
    "        for val in emails: \n",
    "            if val != None : \n",
    "                res.append(val) \n",
    "        if res:        \n",
    "            for i in res:        \n",
    "                match = re.findall(r'([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)',i)\n",
    "                for i in match:\n",
    "                    return i\n",
    "        else:\n",
    "            return \"None\"\n",
    "        #----------------------------------END-----------------------------------------------    \n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 12/12 [00:19<00:00,  1.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Kyle@HealthcareSuccess.com'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_from_web('healthcaresuccess.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
